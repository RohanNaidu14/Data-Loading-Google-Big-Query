{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sqlalchemy import create_engine, text\n",
    "# Set display.max_columns to None to show all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your SQL query\n",
    "query = \"\"\"\n",
    "    your big query query\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling Big Query table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client.from_service_account_json(**path to your big query service account key for authorization**)\n",
    "\n",
    "\n",
    "# Execute the query and fetch results\n",
    "query_job = client.query(query)  # API request\n",
    "\n",
    "df = query_job.to_dataframe(create_bqstorage_client=True)\n",
    "\n",
    "\n",
    "# # Retrieve query results\n",
    "# results = query_job.result()  # Waits for job to complete\n",
    "\n",
    "# for row in results:\n",
    "#     for key, value in row.items():\n",
    "#         print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8805, 30)"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Flattening Code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Event Params Column Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to flatten the event_params\n",
    "def flatten_event_params(row):\n",
    "    flattened = {}\n",
    "    for item in row:\n",
    "        key = item['key']\n",
    "        value_dict = item['value']\n",
    "        # Get the non-null value from the value dict\n",
    "        value = next((v for v in value_dict.values() if v is not None), None)\n",
    "        flattened[f\"event_params_{key}\"] = value\n",
    "    return flattened\n",
    "\n",
    "# Apply the function and expand the results into separate columns\n",
    "flattened_df = pd.DataFrame(df['event_params'].apply(flatten_event_params).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original event_params column\n",
    "df = df.drop(columns=['event_params'])\n",
    "\n",
    "# Concatenate with the original DataFrame if needed\n",
    "df = pd.concat([df, flattened_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8805, 71)"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### privacy_info column flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to flatten the privacy_info column\n",
    "def flatten_privacy_info(row):\n",
    "    flattened = {}\n",
    "    for key, value in row.items():\n",
    "        flattened[f\"privacy_info_{key}\"] = value\n",
    "    return flattened\n",
    "\n",
    "# Apply the function and expand the results into separate columns\n",
    "flattened_privacy_info_df = pd.DataFrame(df['privacy_info'].apply(flatten_privacy_info).tolist())\n",
    "\n",
    "# Drop the original privacy_info column\n",
    "df = df.drop(columns=['privacy_info'])\n",
    "\n",
    "# Concatenate the flattened columns with the original DataFrame\n",
    "df = pd.concat([df, flattened_privacy_info_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8805, 73)"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### user_properties column flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to flatten the user_properties column\n",
    "def flatten_user_properties(row):\n",
    "    flattened = {}\n",
    "    for item in row:\n",
    "        key = item['key']\n",
    "        value_dict = item['value']\n",
    "        \n",
    "        # Pick the correct non-null value based on your rules\n",
    "        value = (\n",
    "            value_dict['string_value'] or \n",
    "            value_dict['int_value'] or \n",
    "            value_dict['float_value'] or \n",
    "            value_dict['double_value']\n",
    "        )\n",
    "        \n",
    "        flattened[f\"user_properties_{key}\"] = value\n",
    "    return flattened\n",
    "\n",
    "# Apply the function and expand the results into separate columns\n",
    "flattened_user_properties_df = pd.DataFrame(df['user_properties'].apply(flatten_user_properties).tolist())\n",
    "\n",
    "flattened_user_properties_df = flattened_user_properties_df.drop(columns=['user_properties_dob', 'user_properties_doj',\n",
    "                                  'user_properties_age', 'user_properties_gc_uf_loyality', 'user_properties_name'])\n",
    "\n",
    "# # Drop the original user_properties column\n",
    "df = df.drop(columns=['user_properties'])\n",
    "\n",
    "# # Concatenate the flattened columns with the original DataFrame\n",
    "df = pd.concat([df, flattened_user_properties_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8805, 82)"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### device column flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to flatten the device column\n",
    "def flatten_device_info(row):\n",
    "    flattened = {}\n",
    "    for key, value in row.items():\n",
    "        flattened[f\"device_{key}\"] = value\n",
    "    return flattened\n",
    "\n",
    "# Apply the function and expand the results into separate columns\n",
    "flattened_device_df = pd.DataFrame(df['device'].apply(flatten_device_info).tolist())\n",
    "\n",
    "# Drop the original device column\n",
    "df = df.drop(columns=['device'])\n",
    "\n",
    "# Concatenate the flattened columns with the original DataFrame\n",
    "df = pd.concat([df, flattened_device_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8805, 96)"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### geo col flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to flatten the geo column\n",
    "def flatten_geo_info(row):\n",
    "    flattened = {}\n",
    "    for key, value in row.items():\n",
    "        flattened[f\"geo_{key}\"] = value\n",
    "    return flattened\n",
    "\n",
    "# Apply the function and expand the results into separate columns\n",
    "flattened_geo_df = pd.DataFrame(df['geo'].apply(flatten_geo_info).tolist())\n",
    "\n",
    "# Drop the original geo column\n",
    "df = df.drop(columns=['geo'])\n",
    "\n",
    "# Concatenate the flattened columns with the original DataFrame\n",
    "df = pd.concat([df, flattened_geo_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8805, 101)"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### app_info col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to flatten the app_info column\n",
    "def flatten_app_info(row):\n",
    "    flattened = {}\n",
    "    for key, value in row.items():\n",
    "        flattened[f\"app_info_{key}\"] = value\n",
    "    return flattened\n",
    "\n",
    "# Apply the function and expand the results into separate columns\n",
    "flattened_app_info_df = pd.DataFrame(df['app_info'].apply(flatten_app_info).tolist())\n",
    "\n",
    "# Drop the original app_info column\n",
    "df = df.drop(columns=['app_info'])\n",
    "\n",
    "# Concatenate the flattened columns with the original DataFrame\n",
    "df = pd.concat([df, flattened_app_info_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8805, 105)"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### traffic_source col "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to flatten the traffic_source column\n",
    "def flatten_traffic_source(row):\n",
    "    flattened = {}\n",
    "    for key, value in row.items():\n",
    "        flattened[f\"traffic_source_{key}\"] = value\n",
    "    return flattened\n",
    "\n",
    "# Apply the function and expand the results into separate columns\n",
    "flattened_traffic_source_df = pd.DataFrame(df['traffic_source'].apply(flatten_traffic_source).tolist())\n",
    "\n",
    "# Drop the original traffic_source column\n",
    "df = df.drop(columns=['traffic_source'])\n",
    "\n",
    "# Concatenate the flattened columns with the original DataFrame\n",
    "df = pd.concat([df, flattened_traffic_source_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8805, 107)"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### items col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['items'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8805, 107)"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace arrays that are empty with None (equivalent to NULL)\n",
    "df['items'] = df['items'].apply(lambda x: None if isinstance(x, np.ndarray) and x.size == 0 else x)\n",
    "\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  collected_traffic_source flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_collected_traffic_source(row):\n",
    "    # Initialize an empty dictionary to hold the flattened data\n",
    "    flattened = {}\n",
    "\n",
    "    # Ensure the row is not None\n",
    "    if row:\n",
    "        # Loop through each key-value pair in the dictionary\n",
    "        for key, value in row.items():\n",
    "            # Only add the key-value pair if the value is not None\n",
    "            flattened[f\"collected_traffic_source_{key}\"] = value\n",
    "\n",
    "    # Ensure that all expected columns are present, even if they are None\n",
    "    expected_columns = [\n",
    "        'manual_campaign_id', 'manual_campaign_name', 'manual_source', 'manual_medium', \n",
    "        'manual_term', 'manual_content', 'manual_source_platform', 'manual_creative_format',\n",
    "        'manual_marketing_tactic', 'gclid', 'dclid', 'srsltid'\n",
    "    ]\n",
    "\n",
    "    for col in expected_columns:\n",
    "        if f\"collected_traffic_source_{col}\" not in flattened:\n",
    "            flattened[f\"collected_traffic_source_{col}\"] = None\n",
    "\n",
    "    return flattened\n",
    "\n",
    "# Apply the function and expand the results into separate columns\n",
    "flattened_collected_traffic_source_df = pd.DataFrame(df['collected_traffic_source'].apply(flatten_collected_traffic_source).tolist())\n",
    "\n",
    "\n",
    "# Drop the original traffic_source column\n",
    "df = df.drop(columns=['collected_traffic_source'])\n",
    "\n",
    "# Concatenate the flattened columns with the original DataFrame\n",
    "df = pd.concat([df, flattened_collected_traffic_source_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### session_traffic_source_last_click col flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_session_traffic_source_last_click(row):\n",
    "    # Initialize an empty dictionary to hold the flattened data\n",
    "    flattened = {}\n",
    "\n",
    "    # Handle the 'manual_campaign' nested dictionary separately\n",
    "    if 'manual_campaign' in row and row['manual_campaign'] is not None:\n",
    "        manual_campaign = row['manual_campaign']\n",
    "        for key, value in manual_campaign.items():\n",
    "            flattened[f\"session_traffic_source_last_click_manual_campaign_{key}\"] = value\n",
    "    else:\n",
    "        # Ensure that all expected keys from 'manual_campaign' are added with None if not available\n",
    "        manual_campaign_keys = [\n",
    "            'campaign_id', 'campaign_name', 'source', 'medium', 'term', 'content', \n",
    "            'source_platform', 'creative_format', 'marketing_tactic'\n",
    "        ]\n",
    "        for key in manual_campaign_keys:\n",
    "            flattened[f\"session_traffic_source_last_click_manual_campaign_{key}\"] = '(not set)'\n",
    "\n",
    "    # Handle the rest of the columns in the row\n",
    "    expected_columns = [\n",
    "        'google_ads_campaign', 'cross_channel_campaign', 'sa360_campaign',\n",
    "        'cm360_campaign', 'dv360_campaign'\n",
    "    ]\n",
    "    \n",
    "    for col in expected_columns:\n",
    "        flattened[f\"session_traffic_source_last_click_{col}\"] = row.get(col, None)\n",
    "\n",
    "    return flattened\n",
    "\n",
    "# Apply the function and expand the results into separate columns\n",
    "flattened_session_traffic_source_last_click_df = pd.DataFrame(df['session_traffic_source_last_click'].apply(flatten_session_traffic_source_last_click).tolist())\n",
    "\n",
    "# Drop the original traffic_source column\n",
    "df = df.drop(columns=['session_traffic_source_last_click'])\n",
    "\n",
    "# Concatenate the flattened columns with the original DataFrame\n",
    "df = pd.concat([df, flattened_session_traffic_source_last_click_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8805, 131)"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date Cols Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of date columns (based on your DataFrame's column names)\n",
    "date_columns = []\n",
    "\n",
    "# Convert date columns to date format\n",
    "for col in date_columns:\n",
    "    df[col] = pd.to_datetime(df[col], errors='coerce').dt.date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ID Cols Transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of ID columns to convert to integer\n",
    "id_columns = []\n",
    "\n",
    "# Convert ID columns to integer format\n",
    "for col in id_columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce', downcast='integer')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time cols Transformation  code below üëáüèº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns containing dicts to JSON format (if they exist)\n",
    "for column in df.columns:\n",
    "    if df[column].apply(lambda x: isinstance(x, dict)).any():\n",
    "        df[column] = df[column].apply(lambda x: json.dumps(x) if isinstance(x, dict) else x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data from final_df to the staging table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(podtgresdb host port username password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace 'your_schema' and 'staging_table' with your actual schema and table names\n",
    "df.to_sql(table_name, engine, schema, if_exists='replace', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
